{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeaa825b",
   "metadata": {
    "papermill": {
     "duration": 0.017165,
     "end_time": "2025-02-02T00:56:45.438006",
     "exception": false,
     "start_time": "2025-02-02T00:56:45.420841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Machine Learning Framework for Stroke Prediction: Balancing Precision and Recall in Healthcare Analytics (Worker Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08938bf5",
   "metadata": {
    "papermill": {
     "duration": 0.004856,
     "end_time": "2025-02-02T00:56:45.449699",
     "exception": false,
     "start_time": "2025-02-02T00:56:45.444843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing the libraries and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9a447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:45.458556Z",
     "iopub.status.busy": "2025-02-02T00:56:45.458418Z",
     "iopub.status.idle": "2025-02-02T00:56:46.363707Z",
     "shell.execute_reply": "2025-02-02T00:56:46.363344Z"
    },
    "papermill": {
     "duration": 0.910674,
     "end_time": "2025-02-02T00:56:46.364654",
     "exception": false,
     "start_time": "2025-02-02T00:56:45.453980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, KMeansSMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, precision_recall_curve, auc, roc_curve, matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "import scrapbook\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bda029",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b210bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.372975Z",
     "iopub.status.busy": "2025-02-02T00:56:46.372841Z",
     "iopub.status.idle": "2025-02-02T00:56:46.382220Z",
     "shell.execute_reply": "2025-02-02T00:56:46.381951Z"
    },
    "papermill": {
     "duration": 0.014156,
     "end_time": "2025-02-02T00:56:46.382998",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.368842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('dataset_train.csv')\n",
    "dataset_val = pd.read_csv('dataset_val.csv')\n",
    "dataset_test = pd.read_csv('dataset_test.csv')\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b67c00",
   "metadata": {
    "papermill": {
     "duration": 0.003397,
     "end_time": "2025-02-02T00:56:46.389924",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.386527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b9c36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.397198Z",
     "iopub.status.busy": "2025-02-02T00:56:46.397075Z",
     "iopub.status.idle": "2025-02-02T00:56:46.399147Z",
     "shell.execute_reply": "2025-02-02T00:56:46.398943Z"
    },
    "papermill": {
     "duration": 0.006438,
     "end_time": "2025-02-02T00:56:46.399799",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.393361",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "imbalanced_action = 2\n",
    "use_PCA = False\n",
    "normalize = True\n",
    "scale = True\n",
    "feature_binning = False\n",
    "k_features = 5\n",
    "feature_selection_mode = 1\n",
    "threshold = 0.5\n",
    "model_type = 10\n",
    "loss_type = 2\n",
    "learning_rate = 0.001\n",
    "model_file_path = '/slaves/models/model_x.pkl'\n",
    "pca_file_path = '/slaves/models/pca_x.pkl'\n",
    "scaler_file_path = '/slaves/models/scaler_x.pkl'\n",
    "normalizer_file_path = '/slaves/models/normalizer_x.pkl'\n",
    "encoder_file_path = '/slaves/models/encoder_x.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42db82e",
   "metadata": {
    "papermill": {
     "duration": 0.003419,
     "end_time": "2025-02-02T00:56:46.416213",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.412794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7367c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.423281Z",
     "iopub.status.busy": "2025-02-02T00:56:46.423140Z",
     "iopub.status.idle": "2025-02-02T00:56:46.426130Z",
     "shell.execute_reply": "2025-02-02T00:56:46.425954Z"
    },
    "papermill": {
     "duration": 0.00724,
     "end_time": "2025-02-02T00:56:46.426754",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.419514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if feature_binning:\n",
    "    age_bins = [0, 30, 45, 60, 80, np.inf]\n",
    "    age_labels = ['Under30', '30-45', '45-60', '60-80', 'Over80']\n",
    "\n",
    "    dataset_train['age_bin'] = pd.cut(dataset_train['age'], bins=age_bins, labels=age_labels)\n",
    "    dataset_val['age_bin'] = pd.cut(dataset_val['age'], bins=age_bins, labels=age_labels)\n",
    "    dataset_test['age_bin'] = pd.cut(dataset_test['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "    if 'bmi' in dataset_train.columns:\n",
    "        bmi_bins = [0, 18.5, 25, 30, np.inf]\n",
    "        bmi_labels = ['Underweight', 'Normal', 'Overweight', 'Obese']\n",
    "\n",
    "        dataset_train['bmi_bin'] = pd.cut(dataset_train['bmi'], bins=bmi_bins, labels=bmi_labels)\n",
    "        dataset_val['bmi_bin'] = pd.cut(dataset_val['bmi'], bins=bmi_bins, labels=bmi_labels)\n",
    "        dataset_test['bmi_bin'] = pd.cut(dataset_test['bmi'], bins=bmi_bins, labels=bmi_labels)\n",
    "        \n",
    "    glucose_bins = [0, 70, 99, 125, np.inf]\n",
    "    glucose_labels = ['Low', 'Normal', 'Prediabetic', 'Diabetic']\n",
    "    dataset_train['glucose_bin'] = pd.cut(dataset_train['avg_glucose_level'], \n",
    "                                    bins=glucose_bins, \n",
    "                                    labels=glucose_labels)\n",
    "    dataset_val['glucose_bin'] = pd.cut(dataset_val['avg_glucose_level'],\n",
    "                                    bins=glucose_bins, \n",
    "                                    labels=glucose_labels)\n",
    "    dataset_test['glucose_bin'] = pd.cut(dataset_test['avg_glucose_level'],\n",
    "                                    bins=glucose_bins, \n",
    "                                    labels=glucose_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3915410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.433831Z",
     "iopub.status.busy": "2025-02-02T00:56:46.433737Z",
     "iopub.status.idle": "2025-02-02T00:56:46.436445Z",
     "shell.execute_reply": "2025-02-02T00:56:46.436266Z"
    },
    "papermill": {
     "duration": 0.007043,
     "end_time": "2025-02-02T00:56:46.437062",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.430019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if feature_binning:\n",
    "    categorical_cols = ['age_bin', 'bmi_bin', 'glucose_bin']\n",
    "    plots_per_row = 3\n",
    "    rows = math.ceil(len(categorical_cols) / plots_per_row)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, plots_per_row, figsize=(5 * plots_per_row, 4 * rows))\n",
    "    axes = np.atleast_2d(axes)\n",
    "\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        r = i // plots_per_row\n",
    "        c = i % plots_per_row\n",
    "        \n",
    "        sns.countplot(x=col, hue='stroke', data=dataset_train, palette='viridis', ax=axes[r][c])\n",
    "        axes[r][c].set_title(f\"Distribution of {col} by Stroke Status\")\n",
    "        axes[r][c].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    total_subplots = rows * plots_per_row\n",
    "    for j in range(i+1, total_subplots):\n",
    "        r = j // plots_per_row\n",
    "        c = j % plots_per_row\n",
    "        axes[r][c].set_visible(False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba611b",
   "metadata": {
    "papermill": {
     "duration": 0.003566,
     "end_time": "2025-02-02T00:56:46.444021",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.440455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### If using the feature binning, we need to get rid of the age and bmi columns (we will use age_bin and bmi_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fb36c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.451330Z",
     "iopub.status.busy": "2025-02-02T00:56:46.451235Z",
     "iopub.status.idle": "2025-02-02T00:56:46.452961Z",
     "shell.execute_reply": "2025-02-02T00:56:46.452777Z"
    },
    "papermill": {
     "duration": 0.006111,
     "end_time": "2025-02-02T00:56:46.453582",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.447471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if feature_binning:\n",
    "    dataset_train = dataset_train.drop(['age', 'avg_glucose_level', 'bmi'], axis=1)\n",
    "    dataset_val = dataset_val.drop(['age', 'avg_glucose_level', 'bmi'], axis=1)\n",
    "    dataset_test = dataset_test.drop(['age', 'avg_glucose_level', 'bmi'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffaa8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.460562Z",
     "iopub.status.busy": "2025-02-02T00:56:46.460471Z",
     "iopub.status.idle": "2025-02-02T00:56:46.463799Z",
     "shell.execute_reply": "2025-02-02T00:56:46.463594Z"
    },
    "papermill": {
     "duration": 0.007569,
     "end_time": "2025-02-02T00:56:46.464425",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.456856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    col for col in dataset_train.columns \n",
    "    if dataset_train[col].dtype == 'object' or dataset_train[col].nunique() <= 10 \n",
    "]\n",
    "\n",
    "\n",
    "numerical_features = [col for col in dataset_train.columns if col not in categorical_features]\n",
    "\n",
    "categorical_features = categorical_features[:-1]\n",
    "\n",
    "categorical_features = [dataset_train.columns.get_loc(col) for col in categorical_features]\n",
    "numerical_features = [dataset_train.columns.get_loc(col) for col in numerical_features]\n",
    "categorical_features, numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97197a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.471891Z",
     "iopub.status.busy": "2025-02-02T00:56:46.471719Z",
     "iopub.status.idle": "2025-02-02T00:56:46.479200Z",
     "shell.execute_reply": "2025-02-02T00:56:46.479003Z"
    },
    "papermill": {
     "duration": 0.011936,
     "end_time": "2025-02-02T00:56:46.479825",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.467889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "for column in dataset_train.select_dtypes(include=['object', 'category']).columns:\n",
    "    dataset_train[column] = encoder.fit_transform(dataset_train[column])\n",
    "    dataset_val[column] = encoder.transform(dataset_val[column])\n",
    "    dataset_test[column] = encoder.transform(dataset_test[column])\n",
    "\n",
    "joblib.dump(encoder, encoder_file_path)\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4824223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.511560Z",
     "iopub.status.busy": "2025-02-02T00:56:46.511381Z",
     "iopub.status.idle": "2025-02-02T00:56:46.514401Z",
     "shell.execute_reply": "2025-02-02T00:56:46.514112Z"
    },
    "papermill": {
     "duration": 0.016693,
     "end_time": "2025-02-02T00:56:46.515144",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.498451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train = dataset_train.drop('stroke', axis=1), dataset_train['stroke']\n",
    "X_val, y_val = dataset_val.drop('stroke', axis=1), dataset_val['stroke']\n",
    "X_test, y_test = dataset_test.drop('stroke', axis=1), dataset_test['stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98235adc",
   "metadata": {
    "papermill": {
     "duration": 0.003594,
     "end_time": "2025-02-02T00:56:46.522633",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.519039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682321a",
   "metadata": {
    "papermill": {
     "duration": 0.003621,
     "end_time": "2025-02-02T00:56:46.529786",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.526165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Combating imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756939b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.537515Z",
     "iopub.status.busy": "2025-02-02T00:56:46.537409Z",
     "iopub.status.idle": "2025-02-02T00:56:46.543769Z",
     "shell.execute_reply": "2025-02-02T00:56:46.543540Z"
    },
    "papermill": {
     "duration": 0.011056,
     "end_time": "2025-02-02T00:56:46.544445",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.533389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if imbalanced_action == 1:\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "elif imbalanced_action == 2:\n",
    "        under = RandomUnderSampler(random_state=42)\n",
    "        X_train, y_train = under.fit_resample(X_train, y_train)\n",
    "elif imbalanced_action == 3:\n",
    "        over = RandomOverSampler(random_state=42)\n",
    "        X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "elif imbalanced_action == 4:\n",
    "        over = SMOTE(sampling_strategy = 1, random_state=42)\n",
    "        under = RandomUnderSampler(sampling_strategy = 0.1, random_state=42)\n",
    "        steps = [('u', under), ('o', over)]\n",
    "        pipeline = Pipeline(steps=steps)\n",
    "        X_train, y_train = pipeline.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6302e3c",
   "metadata": {
    "papermill": {
     "duration": 0.003653,
     "end_time": "2025-02-02T00:56:46.551661",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.548008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Selection of most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83017a",
   "metadata": {
    "papermill": {
     "duration": 0.00364,
     "end_time": "2025-02-02T00:56:46.558810",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.555170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Method 1: via Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0fda5",
   "metadata": {
    "papermill": {
     "duration": 0.003492,
     "end_time": "2025-02-02T00:56:46.565730",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.562238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Checking categorical features first using mutual information score, the Chi index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea32718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.573320Z",
     "iopub.status.busy": "2025-02-02T00:56:46.573147Z",
     "iopub.status.idle": "2025-02-02T00:56:46.575702Z",
     "shell.execute_reply": "2025-02-02T00:56:46.575496Z"
    },
    "papermill": {
     "duration": 0.007171,
     "end_time": "2025-02-02T00:56:46.576342",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.569171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if feature_selection_mode == 0:\n",
    "    colors = 'coolwarm'\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,5))\n",
    "\n",
    "    plt.subplot(1,1,1)\n",
    "    features = X_train.iloc[:, categorical_features]\n",
    "    target = y_train\n",
    "\n",
    "    best_features = SelectKBest(score_func = mutual_info_classif,k = 'all')\n",
    "    fit = best_features.fit(features,target)\n",
    "\n",
    "    featureScores = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['Mutual Information Score']) \n",
    "    sns.heatmap(featureScores.sort_values(ascending = False,by = 'Mutual Information Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\n",
    "    plt.title('Categorical Feature Importances using Mutual Information Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba43471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.583606Z",
     "iopub.status.busy": "2025-02-02T00:56:46.583516Z",
     "iopub.status.idle": "2025-02-02T00:56:46.585864Z",
     "shell.execute_reply": "2025-02-02T00:56:46.585678Z"
    },
    "papermill": {
     "duration": 0.006726,
     "end_time": "2025-02-02T00:56:46.586471",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.579745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if feature_selection_mode == 0:\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,5))\n",
    "\n",
    "    plt.subplot(1,1,1)\n",
    "    features = X_train.iloc[:, categorical_features]\n",
    "    target = y_train\n",
    "\n",
    "    best_features = SelectKBest(score_func = chi2,k = 'all')\n",
    "    fit = best_features.fit(features,target)\n",
    "\n",
    "    featureScores_Chi = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['Chi-Square Test']) \n",
    "    sns.heatmap(featureScores_Chi.sort_values(ascending = False,by = 'Chi-Square Test'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\n",
    "    plt.title('Feature Importances using Chi-Square Test');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10e090",
   "metadata": {
    "papermill": {
     "duration": 0.003875,
     "end_time": "2025-02-02T00:56:46.594294",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.590419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Now checking for the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8658b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.601845Z",
     "iopub.status.busy": "2025-02-02T00:56:46.601752Z",
     "iopub.status.idle": "2025-02-02T00:56:46.604219Z",
     "shell.execute_reply": "2025-02-02T00:56:46.604009Z"
    },
    "papermill": {
     "duration": 0.007093,
     "end_time": "2025-02-02T00:56:46.604962",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.597869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "if feature_selection_mode == 0 and not feature_binning:\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,5))\n",
    "\n",
    "    plt.subplot(1,1,1)\n",
    "    features = X_train.iloc[:,numerical_features]\n",
    "    target = y_train\n",
    "\n",
    "    best_features = SelectKBest(score_func = f_classif,k = 'all')\n",
    "    fit = best_features.fit(features,target)\n",
    "\n",
    "    featureScores_ANOVA = pd.DataFrame(data = fit.scores_,index = list(features.columns),columns = ['ANOVA Score']) \n",
    "    sns.heatmap(featureScores_ANOVA.sort_values(ascending = False,by = 'ANOVA Score'),annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',fmt = '.2f');\n",
    "    plt.title('Selection of Numerical Features');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9915b24",
   "metadata": {
    "papermill": {
     "duration": 0.004135,
     "end_time": "2025-02-02T00:56:46.613304",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.609169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Dropping the columns where the Chi-Square score and ANOVA are not in the top k_features list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594047cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.622278Z",
     "iopub.status.busy": "2025-02-02T00:56:46.622032Z",
     "iopub.status.idle": "2025-02-02T00:56:46.624974Z",
     "shell.execute_reply": "2025-02-02T00:56:46.624700Z"
    },
    "papermill": {
     "duration": 0.008482,
     "end_time": "2025-02-02T00:56:46.625770",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.617288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if feature_selection_mode == 0:\n",
    "    categorical_features_names = X_train.columns[categorical_features]\n",
    "    numerical_features_names = X_train.columns[numerical_features]\n",
    "\n",
    "    chi2_features  = featureScores_Chi.sort_values(ascending = False,by = 'Chi-Square Test').head(k_features).index\n",
    "    if not feature_binning:\n",
    "        anova_features  = featureScores_ANOVA.sort_values(ascending = False,by = 'ANOVA Score').head(k_features).index\n",
    "    else:\n",
    "        anova_features = []\n",
    "    X_train = X_train[chi2_features.union(anova_features)]\n",
    "    X_val = X_val[chi2_features.union(anova_features)]\n",
    "    X_test = X_test[chi2_features.union(anova_features)]\n",
    "\n",
    "    numerical_features = [X_train.columns.get_loc(col) for col in numerical_features_names if col in X_train.columns]\n",
    "    categorical_features = [X_train.columns.get_loc(col) for col in categorical_features_names if col in X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc855eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.634808Z",
     "iopub.status.busy": "2025-02-02T00:56:46.634579Z",
     "iopub.status.idle": "2025-02-02T00:56:46.829416Z",
     "shell.execute_reply": "2025-02-02T00:56:46.828613Z"
    },
    "papermill": {
     "duration": 0.200987,
     "end_time": "2025-02-02T00:56:46.831003",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.630016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if feature_selection_mode == 1:\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    feature_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "    feature_importances.nlargest(10).plot(kind='barh')\n",
    "\n",
    "\n",
    "    most_important_features = feature_importances.nlargest(k_features).index\n",
    "    X_train = X_train[most_important_features]\n",
    "    X_val = X_val[most_important_features]\n",
    "    X_test = X_test[most_important_features]\n",
    "\n",
    "    numerical_features = [X_train.columns.get_loc(col) for col in most_important_features if col in X_train.columns]\n",
    "    categorical_features = [X_train.columns.get_loc(col) for col in most_important_features if col in X_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2bb14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.846252Z",
     "iopub.status.busy": "2025-02-02T00:56:46.845970Z",
     "iopub.status.idle": "2025-02-02T00:56:46.850072Z",
     "shell.execute_reply": "2025-02-02T00:56:46.849506Z"
    },
    "papermill": {
     "duration": 0.01096,
     "end_time": "2025-02-02T00:56:46.851726",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.840766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if normalize and not feature_binning:\n",
    "    normalizer = MinMaxScaler()\n",
    "    X_train.iloc[:, numerical_features] = normalizer.fit_transform(X_train.iloc[:, numerical_features])\n",
    "    X_val.iloc[:, numerical_features] = normalizer.transform(X_val.iloc[:, numerical_features])\n",
    "    X_test.iloc[:, numerical_features] = normalizer.transform(X_test.iloc[:, numerical_features])\n",
    "    joblib.dump(normalizer, normalizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b7862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.866638Z",
     "iopub.status.busy": "2025-02-02T00:56:46.866342Z",
     "iopub.status.idle": "2025-02-02T00:56:46.870096Z",
     "shell.execute_reply": "2025-02-02T00:56:46.869598Z"
    },
    "papermill": {
     "duration": 0.01065,
     "end_time": "2025-02-02T00:56:46.871459",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.860809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if scale and not feature_binning:\n",
    "    scaler = StandardScaler()\n",
    "    X_train.iloc[:, numerical_features] = scaler.fit_transform(X_train.iloc[:, numerical_features])\n",
    "    X_val.iloc[:, numerical_features] = scaler.transform(X_val.iloc[:, numerical_features])\n",
    "    X_test.iloc[:, numerical_features] = scaler.transform(X_test.iloc[:, numerical_features])\n",
    "    joblib.dump(scaler, scaler_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd7b0d",
   "metadata": {
    "papermill": {
     "duration": 0.0098,
     "end_time": "2025-02-02T00:56:46.891922",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.882122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569ad9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.913882Z",
     "iopub.status.busy": "2025-02-02T00:56:46.913337Z",
     "iopub.status.idle": "2025-02-02T00:56:46.919445Z",
     "shell.execute_reply": "2025-02-02T00:56:46.918962Z"
    },
    "papermill": {
     "duration": 0.017105,
     "end_time": "2025-02-02T00:56:46.920899",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.903794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Dense, Dropout, BatchNormalization, Input,\n",
    "                                     Conv1D, Flatten, LSTM, GRU, Bidirectional,\n",
    "                                     LeakyReLU, PReLU, ELU, Add, Concatenate, Lambda,\n",
    "                                     Multiply, Average)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "if model_type == 0:\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_dim=num_features),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 1:\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=num_features),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 2:\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=num_features),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 3:\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=num_features),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 4:\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=num_features),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 5:\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=num_features),\n",
    "        BatchNormalization(),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 6:\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=num_features),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 7:\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.01), input_dim=num_features),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 8:\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=num_features),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dense(32),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 9:\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=num_features),\n",
    "        PReLU(),\n",
    "        Dense(32),\n",
    "        PReLU(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 10:\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=num_features),\n",
    "        ELU(alpha=1.0),\n",
    "        Dense(32),\n",
    "        ELU(alpha=1.0),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 11:\n",
    "    input_layer = Input(shape=(num_features, 1))\n",
    "    conv = Conv1D(32, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "    flat = Flatten()(conv)\n",
    "    output = Dense(1, activation='sigmoid')(flat)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "elif model_type == 12:\n",
    "    input_layer = Input(shape=(num_features, 1))\n",
    "    lstm = LSTM(32)(input_layer)\n",
    "    output = Dense(1, activation='sigmoid')(lstm)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 13:\n",
    "    input_layer = Input(shape=(num_features, 1))\n",
    "    gru = GRU(32)(input_layer)\n",
    "    output = Dense(1, activation='sigmoid')(gru)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 14:\n",
    "    input_layer = Input(shape=(num_features, 1))\n",
    "    bi_lstm = Bidirectional(LSTM(32))(input_layer)\n",
    "    output = Dense(1, activation='sigmoid')(bi_lstm)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 15:\n",
    "    input_layer = Input(shape=(num_features, 1))\n",
    "    lstm1 = LSTM(64, return_sequences=True)(input_layer)\n",
    "    lstm2 = LSTM(32)(lstm1)\n",
    "    output = Dense(1, activation='sigmoid')(lstm2)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 16:\n",
    "    input_layer = Input(shape=(num_features, 1))\n",
    "    gru1 = GRU(64, return_sequences=True)(input_layer)\n",
    "    gru2 = GRU(32)(gru1)\n",
    "    output = Dense(1, activation='sigmoid')(gru2)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 17:\n",
    "    input_layer = Input(shape=(num_features, 1))\n",
    "    lstm_out = LSTM(64, return_sequences=True)(input_layer)\n",
    "    attention = Dense(1, activation='tanh')(lstm_out)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Dense(num_features, activation='softmax')(attention)\n",
    "    attention = Lambda(lambda x: K.expand_dims(x, axis=-1))(attention)\n",
    "    context = Lambda(lambda x: x[0] * x[1])([lstm_out, attention])\n",
    "    context = Lambda(lambda x: K.sum(x, axis=1))(context)\n",
    "    output = Dense(1, activation='sigmoid')(context)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 18:\n",
    "    from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "    input_layer = Input(shape=(num_features, 1))\n",
    "    proj = Dense(64)(input_layer)\n",
    "    attn = MultiHeadAttention(num_heads=4, key_dim=16)(proj, proj)\n",
    "    attn = LayerNormalization()(attn + proj)\n",
    "    flat = Flatten()(attn)\n",
    "    output = Dense(1, activation='sigmoid')(flat)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 19:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    attention_probs = Dense(num_features, activation='softmax')(input_layer)\n",
    "    attended = Multiply()([input_layer, attention_probs])\n",
    "    dense = Dense(64, activation='relu')(attended)\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 20:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    wide = Dense(1, activation='linear')(input_layer)\n",
    "    deep = Dense(64, activation='relu')(input_layer)\n",
    "    deep = Dense(32, activation='relu')(deep)\n",
    "    deep = Dense(1, activation='linear')(deep)\n",
    "    combined = Add()([wide, deep])\n",
    "    output = Dense(1, activation='sigmoid')(combined)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 21:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    x = Dense(64, activation='relu')(input_layer)\n",
    "    shortcut = x\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 22:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    x1 = Dense(32, activation='relu')(input_layer)\n",
    "    x2 = Dense(32, activation='relu')(Concatenate()([input_layer, x1]))\n",
    "    x3 = Dense(32, activation='relu')(Concatenate()([input_layer, x1, x2]))\n",
    "    output = Dense(1, activation='sigmoid')(x3)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 23:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    branch1 = Dense(32, activation='relu')(input_layer)\n",
    "    branch2 = Dense(32, activation='relu')(input_layer)\n",
    "    branch3 = Dense(32, activation='relu')(input_layer)\n",
    "    merged = Concatenate()([branch1, branch2, branch3])\n",
    "    output = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 24:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    encoded = Dense(32, activation='relu')(input_layer)\n",
    "    classifier = Dense(16, activation='relu')(encoded)\n",
    "    output = Dense(1, activation='sigmoid')(classifier)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 25:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    hidden = Dense(64, activation='relu')(input_layer)\n",
    "    z_mean = Dense(16)(hidden)\n",
    "    z_log_var = Dense(16)(hidden)\n",
    "    latent = z_mean\n",
    "    output = Dense(1, activation='sigmoid')(latent)\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "elif model_type == 26:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    shared = Dense(64, activation='relu')(input_layer)\n",
    "    task1 = Dense(32, activation='relu')(shared)\n",
    "    output1 = Dense(1, activation='sigmoid', name='stroke')(task1)\n",
    "    task2 = Dense(32, activation='relu')(shared)\n",
    "    output2 = Dense(1, activation='sigmoid', name='other')(task2)\n",
    "    model = Model(inputs=input_layer, outputs=[output1, output2])\n",
    "\n",
    "elif model_type == 27:\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=num_features),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "elif model_type == 28:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    branch1 = Dense(64, activation='relu')(input_layer)\n",
    "    branch1 = Dense(1, activation='sigmoid')(branch1)\n",
    "    branch2 = Dense(32, activation='relu')(input_layer)\n",
    "    branch2 = Dense(1, activation='sigmoid')(branch2)\n",
    "    ensemble_output = Average()([branch1, branch2])\n",
    "    model = Model(inputs=input_layer, outputs=ensemble_output)\n",
    "elif model_type == 29:\n",
    "    input_layer = Input(shape=(num_features,))\n",
    "    mlp = Dense(64, activation='relu')(input_layer)\n",
    "    mlp = Dense(32, activation='relu')(mlp)\n",
    "    reshaped = Lambda(lambda x: K.expand_dims(x, axis=-1))(input_layer)\n",
    "    cnn = Conv1D(32, kernel_size=3, activation='relu', padding='same')(reshaped)\n",
    "    cnn = Flatten()(cnn)\n",
    "    combined = Concatenate()([mlp, cnn])\n",
    "    output = Dense(1, activation='sigmoid')(combined)\n",
    "    model = Model(inputs=input_layer, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c561e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, y_pred.dtype)\n",
    "        loss = -weights[1] * y_true * K.log(y_pred + K.epsilon()) - \\\n",
    "               weights[0] * (1 - y_true) * K.log(1 - y_pred + K.epsilon())\n",
    "        return K.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1. - K.epsilon())\n",
    "        loss_value = -y_true * alpha * K.pow(1 - y_pred, gamma) * K.log(y_pred) - \\\n",
    "                     (1 - y_true) * (1 - alpha) * K.pow(y_pred, gamma) * K.log(1 - y_pred)\n",
    "        return K.mean(loss_value)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ca6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tversky_loss(y_true, y_pred, alpha=0.5, beta=0.5, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    tp = K.sum(y_true_f * y_pred_f)\n",
    "    fn = K.sum(y_true_f * (1 - y_pred_f))\n",
    "    fp = K.sum((1 - y_true_f) * y_pred_f)\n",
    "    tversky_index = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth)\n",
    "    return 1 - tversky_index\n",
    "\n",
    "def focal_tversky_loss(y_true, y_pred, alpha=0.5, beta=0.5, gamma=1.0, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    tp = K.sum(y_true_f * y_pred_f)\n",
    "    fn = K.sum(y_true_f * (1 - y_pred_f))\n",
    "    fp = K.sum((1 - y_true_f) * y_pred_f)\n",
    "    tversky_index = (tp + smooth) / (tp + alpha * fn + beta * fp + smooth)\n",
    "    return K.pow((1 - tversky_index), gamma)\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice_coeff = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1 - dice_coeff\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    d_loss = dice_loss(y_true, y_pred, smooth)\n",
    "    return bce + d_loss\n",
    "\n",
    "def fbeta_loss(y_true, y_pred, beta=2, smooth=1e-6):\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    tp = K.sum(y_true_f * y_pred_f)\n",
    "    fp = K.sum((1 - y_true_f) * y_pred_f)\n",
    "    fn = K.sum(y_true_f * (1 - y_pred_f))\n",
    "    fbeta = (1 + beta**2) * tp / ((1 + beta**2) * tp + beta**2 * fn + fp + smooth)\n",
    "    return 1 - fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2dba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "y_train_np = np.array(y_train).flatten()\n",
    "\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_np),\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "class_weights = {i: weight for i, weight in enumerate(weights)}\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbda7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:46.984364Z",
     "iopub.status.busy": "2025-02-02T00:56:46.984159Z",
     "iopub.status.idle": "2025-02-02T00:56:47.476548Z",
     "shell.execute_reply": "2025-02-02T00:56:47.476294Z"
    },
    "papermill": {
     "duration": 0.498655,
     "end_time": "2025-02-02T00:56:47.477313",
     "exception": false,
     "start_time": "2025-02-02T00:56:46.978658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    \n",
    "    y_pred = K.round(y_pred)\n",
    "    \n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    fp = K.sum((1 - y_true) * y_pred, axis=0)\n",
    "    fn = K.sum(y_true * (1 - y_pred), axis=0)\n",
    "    \n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "    beta = 2\n",
    "    f2 = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + K.epsilon())\n",
    "    return K.mean(f2)\n",
    "\n",
    "if loss_type == 0:\n",
    "    loss = 'binary_crossentropy'\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=[f2_metric])\n",
    "elif loss_type == 1:\n",
    "    loss = weighted_binary_crossentropy(weights=class_weights)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=[f2_metric])\n",
    "elif loss_type == 2:\n",
    "    loss = focal_loss(gamma=2., alpha=0.25)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=[f2_metric])\n",
    "elif loss_type == 3:\n",
    "    loss = tversky_loss\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=[f2_metric])\n",
    "elif loss_type == 4:\n",
    "    loss = focal_tversky_loss\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=[f2_metric])\n",
    "elif loss_type == 5:\n",
    "    loss = bce_dice_loss\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=[f2_metric])\n",
    "elif loss_type == 6:\n",
    "    loss = fbeta_loss\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=[f2_metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee086596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, CSVLogger, TerminateOnNaN, LearningRateScheduler\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "    TerminateOnNaN(),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=1000, \n",
    "    batch_size=128, \n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988c6af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.485561Z",
     "iopub.status.busy": "2025-02-02T00:56:47.485459Z",
     "iopub.status.idle": "2025-02-02T00:56:47.526022Z",
     "shell.execute_reply": "2025-02-02T00:56:47.525798Z"
    },
    "papermill": {
     "duration": 0.04535,
     "end_time": "2025-02-02T00:56:47.526731",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.481381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_probabilities = model.predict(X_val)\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_probabilities > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9225014",
   "metadata": {
    "papermill": {
     "duration": 0.004236,
     "end_time": "2025-02-02T00:56:47.535458",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.531222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Metrics on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b31dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.543777Z",
     "iopub.status.busy": "2025-02-02T00:56:47.543662Z",
     "iopub.status.idle": "2025-02-02T00:56:47.714586Z",
     "shell.execute_reply": "2025-02-02T00:56:47.714273Z"
    },
    "papermill": {
     "duration": 0.175953,
     "end_time": "2025-02-02T00:56:47.715355",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.539402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "threshold = 0.5\n",
    "y_pred_train = (y_pred_train > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c46482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.723768Z",
     "iopub.status.busy": "2025-02-02T00:56:47.723666Z",
     "iopub.status.idle": "2025-02-02T00:56:47.729003Z",
     "shell.execute_reply": "2025-02-02T00:56:47.728785Z"
    },
    "papermill": {
     "duration": 0.010214,
     "end_time": "2025-02-02T00:56:47.729641",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.719427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "print(f\"Train Precision: {precision_train:.2f}\")\n",
    "scrapbook.glue(\"precision_train\", precision_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7594c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.738009Z",
     "iopub.status.busy": "2025-02-02T00:56:47.737918Z",
     "iopub.status.idle": "2025-02-02T00:56:47.741797Z",
     "shell.execute_reply": "2025-02-02T00:56:47.741562Z"
    },
    "papermill": {
     "duration": 0.008752,
     "end_time": "2025-02-02T00:56:47.742405",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.733653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "print(f\"Train Recall: {recall_train:.2f}\")\n",
    "scrapbook.glue(\"recall_train\", recall_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000d054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.751633Z",
     "iopub.status.busy": "2025-02-02T00:56:47.751531Z",
     "iopub.status.idle": "2025-02-02T00:56:47.755505Z",
     "shell.execute_reply": "2025-02-02T00:56:47.755289Z"
    },
    "papermill": {
     "duration": 0.00916,
     "end_time": "2025-02-02T00:56:47.756133",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.746973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_score_train = f1_score(y_train, y_pred_train)\n",
    "print(f\"Train F1 Score: {f1_score_train:.2f}\")\n",
    "scrapbook.glue(\"f1_score_train\", f1_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9defee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.765031Z",
     "iopub.status.busy": "2025-02-02T00:56:47.764928Z",
     "iopub.status.idle": "2025-02-02T00:56:47.768171Z",
     "shell.execute_reply": "2025-02-02T00:56:47.767962Z"
    },
    "papermill": {
     "duration": 0.008634,
     "end_time": "2025-02-02T00:56:47.768825",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.760191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score_train = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Train Accuracy: {accuracy_score_train:.2f}\")\n",
    "scrapbook.glue(\"accuracy_score_train\", accuracy_score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ac79e",
   "metadata": {
    "papermill": {
     "duration": 0.00422,
     "end_time": "2025-02-02T00:56:47.777103",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.772883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation Metrics on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a9a3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.785732Z",
     "iopub.status.busy": "2025-02-02T00:56:47.785632Z",
     "iopub.status.idle": "2025-02-02T00:56:47.790734Z",
     "shell.execute_reply": "2025-02-02T00:56:47.790516Z"
    },
    "papermill": {
     "duration": 0.01024,
     "end_time": "2025-02-02T00:56:47.791380",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.781140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_pred)\n",
    "scrapbook.glue(\"classification_report\", report)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af035be6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.800478Z",
     "iopub.status.busy": "2025-02-02T00:56:47.800386Z",
     "iopub.status.idle": "2025-02-02T00:56:47.852887Z",
     "shell.execute_reply": "2025-02-02T00:56:47.852660Z"
    },
    "papermill": {
     "duration": 0.057806,
     "end_time": "2025-02-02T00:56:47.853582",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.795776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Confusion Matrix on Validation Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "scrapbook.glue(\"confusion_matrix\", plt.gcf(), encoder='display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfbda6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.864239Z",
     "iopub.status.busy": "2025-02-02T00:56:47.864140Z",
     "iopub.status.idle": "2025-02-02T00:56:47.868040Z",
     "shell.execute_reply": "2025-02-02T00:56:47.867801Z"
    },
    "papermill": {
     "duration": 0.009755,
     "end_time": "2025-02-02T00:56:47.868694",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.858939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision = precision_score(y_val, y_pred)\n",
    "print(f\"Precision: {precision}\")\n",
    "scrapbook.glue(\"precision\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798738e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.879440Z",
     "iopub.status.busy": "2025-02-02T00:56:47.879329Z",
     "iopub.status.idle": "2025-02-02T00:56:47.883298Z",
     "shell.execute_reply": "2025-02-02T00:56:47.883091Z"
    },
    "papermill": {
     "duration": 0.010282,
     "end_time": "2025-02-02T00:56:47.883917",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.873635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall = recall_score(y_val, y_pred)\n",
    "print(f\"Recall: {recall}\")\n",
    "scrapbook.glue(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00ff3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.894024Z",
     "iopub.status.busy": "2025-02-02T00:56:47.893925Z",
     "iopub.status.idle": "2025-02-02T00:56:47.897691Z",
     "shell.execute_reply": "2025-02-02T00:56:47.897486Z"
    },
    "papermill": {
     "duration": 0.009511,
     "end_time": "2025-02-02T00:56:47.898330",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.888819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_score_v = f1_score(y_val, y_pred)\n",
    "print(f\"F1 Score: {f1_score_v}\")\n",
    "scrapbook.glue(\"f1_score\", f1_score_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66386246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.908612Z",
     "iopub.status.busy": "2025-02-02T00:56:47.908511Z",
     "iopub.status.idle": "2025-02-02T00:56:47.911722Z",
     "shell.execute_reply": "2025-02-02T00:56:47.911528Z"
    },
    "papermill": {
     "duration": 0.008982,
     "end_time": "2025-02-02T00:56:47.912312",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.903330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "scrapbook.glue(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa020c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = matthews_corrcoef(y_val, y_pred)\n",
    "print(f\"Matthews correlation coefficient: {mcc}\")\n",
    "scrapbook.glue(\"mcc\", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74b04f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:47.922669Z",
     "iopub.status.busy": "2025-02-02T00:56:47.922571Z",
     "iopub.status.idle": "2025-02-02T00:56:47.975912Z",
     "shell.execute_reply": "2025-02-02T00:56:47.975657Z"
    },
    "papermill": {
     "duration": 0.059347,
     "end_time": "2025-02-02T00:56:47.976657",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.917310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_val, y_pred_probabilities)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "scrapbook.glue(\"precision_recall_curve\", plt.gcf(), encoder='display')\n",
    "scrapbook.glue(\"pr_auc\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0bf9e",
   "metadata": {
    "papermill": {
     "duration": 0.005431,
     "end_time": "2025-02-02T00:56:48.002162",
     "exception": false,
     "start_time": "2025-02-02T00:56:47.996731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Computing validation metrics using best threshold for F1-score (balanced recall and precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8adaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n",
    "\n",
    "thresholds_ = thresholds[~np.isnan(f1_scores)]\n",
    "f1_scores = f1_scores[~np.isnan(f1_scores)]\n",
    "\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds_[best_idx]\n",
    "\n",
    "print(f\"The threshold that maximizes F1 (and thus balances precision and recall) is: {best_threshold}\")\n",
    "scrapbook.glue(\"best_threshold_f1\", float(best_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db131269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:48.013003Z",
     "iopub.status.busy": "2025-02-02T00:56:48.012893Z",
     "iopub.status.idle": "2025-02-02T00:56:48.054863Z",
     "shell.execute_reply": "2025-02-02T00:56:48.054618Z"
    },
    "papermill": {
     "duration": 0.048253,
     "end_time": "2025-02-02T00:56:48.055605",
     "exception": false,
     "start_time": "2025-02-02T00:56:48.007352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred_probabilties = model.predict(X_val)\n",
    "y_pred = (y_pred_probabilties > best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71261b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:48.066794Z",
     "iopub.status.busy": "2025-02-02T00:56:48.066696Z",
     "iopub.status.idle": "2025-02-02T00:56:48.120346Z",
     "shell.execute_reply": "2025-02-02T00:56:48.120103Z"
    },
    "papermill": {
     "duration": 0.059969,
     "end_time": "2025-02-02T00:56:48.121076",
     "exception": false,
     "start_time": "2025-02-02T00:56:48.061107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Confusion Matrix on Validation Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "scrapbook.glue(\"confusion_matrix_f1\", plt.gcf(), encoder='display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb1b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:48.134086Z",
     "iopub.status.busy": "2025-02-02T00:56:48.133968Z",
     "iopub.status.idle": "2025-02-02T00:56:48.138268Z",
     "shell.execute_reply": "2025-02-02T00:56:48.138026Z"
    },
    "papermill": {
     "duration": 0.011544,
     "end_time": "2025-02-02T00:56:48.139021",
     "exception": false,
     "start_time": "2025-02-02T00:56:48.127477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_ = precision_score(y_val, y_pred)\n",
    "print(f\"Precision (Optimal): {precision_}\")\n",
    "scrapbook.glue(\"precision_optimal_f1\", precision_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9ccf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:48.151081Z",
     "iopub.status.busy": "2025-02-02T00:56:48.150963Z",
     "iopub.status.idle": "2025-02-02T00:56:48.155006Z",
     "shell.execute_reply": "2025-02-02T00:56:48.154772Z"
    },
    "papermill": {
     "duration": 0.010627,
     "end_time": "2025-02-02T00:56:48.155680",
     "exception": false,
     "start_time": "2025-02-02T00:56:48.145053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_ = recall_score(y_val, y_pred)\n",
    "print(f\"Recall (Optimal): {recall_}\")\n",
    "scrapbook.glue(\"recall_optimal_f1\", recall_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc2c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T00:56:48.167414Z",
     "iopub.status.busy": "2025-02-02T00:56:48.167312Z",
     "iopub.status.idle": "2025-02-02T00:56:48.171172Z",
     "shell.execute_reply": "2025-02-02T00:56:48.170943Z"
    },
    "papermill": {
     "duration": 0.010432,
     "end_time": "2025-02-02T00:56:48.171828",
     "exception": false,
     "start_time": "2025-02-02T00:56:48.161396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1_score_v = f1_score(y_val, y_pred)\n",
    "print(f\"F1 Score Best: {f1_score_v}\")\n",
    "scrapbook.glue(\"f1_score_optimal\", f1_score_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcb02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = matthews_corrcoef(y_val, y_pred)\n",
    "print(f\"Matthews correlation coefficient: {mcc}\")\n",
    "scrapbook.glue(\"mcc_optimal_f1\", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy (Optimal): {accuracy}\")\n",
    "scrapbook.glue(\"accuracy_optimal_f1\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c483a9",
   "metadata": {},
   "source": [
    "## Computing validation metrics using best threshold for F2-score (recall more important than precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03403172",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_scores = 5 * (precision[:-1] * recall[:-1]) / (4 * precision[:-1] + recall[:-1])\n",
    "\n",
    "thresholds_ = thresholds[~np.isnan(f2_scores)]\n",
    "f2_scores = f2_scores[~np.isnan(f2_scores)]\n",
    "\n",
    "best_idx = np.argmax(f2_scores)\n",
    "best_threshold = thresholds_[best_idx]\n",
    "\n",
    "print(f\"The threshold that maximizes F2 (more recall importance than precision) is: {best_threshold}\")\n",
    "scrapbook.glue(\"best_threshold_f2\", float(best_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probabilties = model.predict(X_val)\n",
    "y_pred = (y_pred_probabilties > best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Confusion Matrix on Validation Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "scrapbook.glue(\"confusion_matrix_f2\", plt.gcf(), encoder='display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec139249",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_ = precision_score(y_val, y_pred)\n",
    "print(f\"Precision (Optimal): {precision_}\")\n",
    "scrapbook.glue(\"precision_optimal_f2\", precision_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ae732",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_ = recall_score(y_val, y_pred)\n",
    "print(f\"Recall (Optimal): {recall_}\")\n",
    "scrapbook.glue(\"recall_optimal_f2\", recall_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14d5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_score = 5 * (precision_ * recall_) / (4 * precision_ + recall_)\n",
    "print(f\"F2 Score Best: {f2_score}\")\n",
    "scrapbook.glue(\"f2_score_optimal_thres\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = matthews_corrcoef(y_val, y_pred)\n",
    "print(f\"Matthews correlation coefficient: {mcc}\")\n",
    "scrapbook.glue(\"mcc_optimal_f2\", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "scrapbook.glue(\"accuracy_optimal_f2\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c597f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20d3d6",
   "metadata": {},
   "source": [
    "## Computing test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb9984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probabilties = model.predict(X_test)\n",
    "y_pred = (y_pred_probabilties > best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b010e5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "scrapbook.glue(\"confusion_matrix_test\", plt.gcf(), encoder='display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2926e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_ = precision_score(y_test, y_pred)\n",
    "print(f\"Precision (Test): {precision_}\")\n",
    "scrapbook.glue(\"precision_test\", precision_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_ = recall_score(y_test, y_pred)\n",
    "print(f\"Recall (Test): {recall_}\")\n",
    "scrapbook.glue(\"recall_test\", recall_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c9ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_v = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score (Test): {f1_score_v}\")\n",
    "scrapbook.glue(\"f1_score_test\", f1_score_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2_score = 5 * (precision_ * recall_) / (4 * precision_ + recall_)\n",
    "print(f\"F2 Score (Test): {f2_score}\")\n",
    "scrapbook.glue(\"f2_score_test\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_test = matthews_corrcoef(y_test, y_pred)\n",
    "print(f\"Matthews correlation coefficient: {mcc_test}\")\n",
    "scrapbook.glue(\"mcc_test\", mcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy (Test): {accuracy_score_test}\")\n",
    "scrapbook.glue(\"accuracy_score_test\", accuracy_score_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thief-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.923028,
   "end_time": "2025-02-02T00:56:50.839047",
   "environment_variables": {},
   "exception": null,
   "input_path": "strok_detection_worker copy_after_b.ipynb",
   "output_path": "workers/notebooks/final_models/stroke_model_820.ipynb",
   "parameters": {
    "feature_binning": false,
    "feature_selection_mode": 1,
    "imbalanced_action": 4,
    "k_features": 4,
    "model_file_path": "workers/models/final_models/stroke_model_820.pkl",
    "model_type": 1,
    "normalize": false,
    "normalizer_file_path": "workers/models/final_models/stroke_normalizer_820.pkl",
    "pca_file_path": "workers/models/final_models/stroke_pca_820.pkl",
    "scale": false,
    "scaler_file_path": "workers/models/final_models/stroke_scaler_820.pkl",
    "threshold": 0.5,
    "use_PCA": false
   },
   "start_time": "2025-02-02T00:56:44.916019",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
